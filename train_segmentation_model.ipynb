{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a UNet segmentation model on the Severstal steel defect dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Dataset generator so that we can easily oversample different classes\n",
    "# Turn on contrast and brightness augmentations\n",
    "# Train full classification model (i.e. don't freeze weights) to see if it does any/much better\n",
    "# Option to resume training (without breaking tensorboard)\n",
    "# Resize before feeding into model, don't let the width get quite so small\n",
    "# Efficient per-class postprocessing grid search\n",
    "#    - Run model once, figure out score for each class for a range of values\n",
    "#    - At the end, pick the best values\n",
    "# Cropping augmentations\n",
    "# Alternative loss functions?\n",
    "#    - Weight boundary pixels more heavily\n",
    "#    - Are class weights necessary?\n",
    "#    - BCE + dice loss\n",
    "#    - Jacquard? Others?\n",
    "#    - Focal loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from steel_seg.utils import (\n",
    "    dice_coeff_kaggle,\n",
    "    per_class_dice_coeff,\n",
    "    rle_to_dense,\n",
    "    dense_to_rle,\n",
    "    visualize_segmentations,\n",
    "    onehottify)\n",
    "from steel_seg.dataset.severstal_steel_dataset import SeverstalSteelDataset\n",
    "from steel_seg.model.unet import build_unet_model\n",
    "from steel_seg.model.deep_q_postprocessor import build_deep_q_model\n",
    "from steel_seg.train import (\n",
    "    class_weighted_binary_crossentropy,\n",
    "    weighted_binary_crossentropy,\n",
    "    pixel_map_weighted_binary_crossentropy,\n",
    "    dice_loss_multi_class,\n",
    "    dice_coef,\n",
    "    DiceCoefByClassAndEmptiness,\n",
    "    eval)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeverstalSteelDataset.init_from_config('SETTINGS.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_batches = dataset.create_dataset(dataset_type='training')\n",
    "val_data, val_batches = dataset.create_dataset(dataset_type='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_model = build_unet_model(\n",
    "#     img_height=cfg['IMG_HEIGHT'],\n",
    "#     img_width=cfg['IMG_WIDTH'],\n",
    "#     img_channels=1,\n",
    "#     num_classes=cfg['NUM_CLASSES'],\n",
    "#     num_layers=4,\n",
    "#     activation=tf.keras.activations.elu,\n",
    "#     kernel_initializer='he_normal',\n",
    "#     kernel_size=(3, 3),\n",
    "#     pool_size=(2, 4),\n",
    "#     num_features=[8, 16, 32, 64],\n",
    "#     drop_prob=0.5)\n",
    "#model_checkpoint_name = 'basic'\n",
    "#checkpoints/cp_20190910-230338.ckpt\n",
    "#checkpoints/cp_20190913-090408.ckpt\n",
    "\n",
    "seg_model = build_unet_model(\n",
    "    img_height=cfg['IMG_HEIGHT'],\n",
    "    img_width=cfg['IMG_WIDTH'],\n",
    "    img_channels=1,\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 4),\n",
    "    num_features=[32, 64, 128, 256],\n",
    "    drop_prob=0.5)\n",
    "model_checkpoint_name = 'deep'\n",
    "# # checkpoints/cp_20190914-142429.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_imgs = dataset.get_image_list('training')\n",
    "\n",
    "# cls_pixel_counts = np.array([0, 0, 0, 0], dtype=np.float64)\n",
    "# total_pixels = 0.0\n",
    "# for img_name in train_imgs:\n",
    "#     img, ann = dataset.get_example_from_img_name(img_name)\n",
    "#     img_pixel_counts = np.sum(ann, axis=(0, 1))\n",
    "#     cls_pixel_counts += img_pixel_counts\n",
    "#     total_pixels += ann.size\n",
    "# cls_pixel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_weights = total_pixels / cls_pixel_counts\n",
    "# cls_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_weights = [5274.42494602, 26340.24368548, 158.51888478, 752.96782738]\n",
    "#cls_weights = np.array([5274.42494602, 26340.24368548, 158.51888478, 752.96782738]) / 20\n",
    "#cls_weights = [263.7212473, 1317.01218427, 7.92594424, 37.64839137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_2_weight = 10.0\n",
    "# cls_0_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[0]\n",
    "# cls_1_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[1]\n",
    "# cls_3_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[3]\n",
    "# cls_weights = [cls_0_weight, cls_1_weight, cls_2_weight, cls_3_weight]\n",
    "#cls_weights = [332.7316460371491, 1661.6470474376874, 10.0, 47.50019711767978]\n",
    "cls_weights = [30.0, 40.0, 10.0, 20.0]\n",
    "#cls_weights = [40.0, 50.0, 5.0, 20.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.0001),\n",
    "    loss=pixel_map_weighted_binary_crossentropy(cls_weights), #dice_loss_multi_class,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(),\n",
    "        dice_coef(batch_size=cfg['BATCH_SIZE']),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=0, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls0'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=1, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls1'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=2, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls2'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=3, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls3'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=0, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls0'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=1, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls1'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=2, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls2'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=3, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls3'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Initial Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = None\n",
    "date_str = '20190916-092052'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('No checkpoints found. Starting from scratch.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    seg_model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use new model name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_name = 'deep_w_aug'\n",
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "initial_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val__dice_coef',#'val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='max',#'auto',\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "logdir = f'logs/{checkpoint_name}-{initial_epoch}'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "    checkpoint_cb,\n",
    "]\n",
    "\n",
    "results = seg_model.fit(\n",
    "    train_data,\n",
    "    epochs=400,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data,\n",
    "    steps_per_epoch=train_batches,\n",
    "    validation_steps=val_batches,\n",
    "    validation_freq=3,\n",
    "    initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = dataset.get_image_list('validation')\n",
    "len(val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_binary_cross_entropy(y_pred, y_true):\n",
    "    assert len(y_true.shape) == 3\n",
    "    eps = 0.000001\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    per_pixel_class_cross_entropy = \\\n",
    "        y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "    per_class_cross_entropy = np.mean(per_pixel_class_cross_entropy, axis=(0, 1))\n",
    "    return per_class_cross_entropy\n",
    "\n",
    "def per_class_iou_score(y_pred, y_true):\n",
    "    assert len(y_true.shape) == 3\n",
    "    intersection = np.sum(y_pred * y_true, axis=(0, 1))\n",
    "    union = np.sum(y_pred, axis=(0, 1)) + np.sum(y_true, axis=(0, 1)) - intersection\n",
    "    \n",
    "    iou = []\n",
    "    for i in range(intersection.shape[0]):\n",
    "        if union[i] == 0:\n",
    "            iou.append(1.0) # Both y_pred and y_true were empty\n",
    "        else:\n",
    "            iou.append(intersection[i] / union[i])\n",
    "    return np.array(iou)\n",
    "\n",
    "def per_class_mask_not_empty(y_true):\n",
    "    return np.sum(y_true, axis=(0, 1)) > 0.5\n",
    "\n",
    "def eval_segmentation(\n",
    "    model,\n",
    "    dataset,\n",
    "    img_list,\n",
    "    thresholds=None,\n",
    "    num_classes=4):\n",
    "    \n",
    "    binary_cross_entropy = np.zeros((len(img_list), num_classes), dtype=np.float32)\n",
    "    dice_coeff = np.zeros((len(img_list), num_classes), dtype=np.float32)\n",
    "    iou_score = np.zeros((len(img_list), num_classes), dtype=np.float32)\n",
    "    mask_not_empty = np.zeros((len(img_list), num_classes), dtype=np.bool)\n",
    "    \n",
    "    if thresholds is None:\n",
    "        thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "    thresholds = np.array(thresholds)\n",
    "\n",
    "    for i, img_name in enumerate(img_list):\n",
    "        img, ann = dataset.get_example_from_img_name(img_name)\n",
    "        img_batch = np.expand_dims(img, axis=0)\n",
    "        y = model.predict(img_batch)\n",
    "        \n",
    "        # Binarize predictions\n",
    "        y_bin = np.zeros_like(y, dtype=np.uint8)\n",
    "        y_bin[y > thresholds] = 1\n",
    "\n",
    "        binary_cross_entropy[i, :] = per_class_binary_cross_entropy(y[0, :, :, :], ann)\n",
    "        dice_coeff[i, :] = per_class_dice_coeff(y_bin[0, :, :, :], ann)\n",
    "        iou_score[i, :] = per_class_iou_score(y_bin[0, :, :, :], ann)\n",
    "        mask_not_empty[i, :] = per_class_mask_not_empty(ann)\n",
    "        \n",
    "    return binary_cross_entropy, dice_coeff, iou_score, mask_not_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy, dice_coeff, iou_score, mask_not_empty = eval_segmentation(\n",
    "    seg_model,\n",
    "    dataset,\n",
    "    val_imgs,\n",
    "    thresholds=[0.9, 0.9, 0.9, 0.9],\n",
    "    num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean dice coeff: {np.mean(dice_coeff)}')\n",
    "print(f'Mean binary cross entropy: {np.mean(binary_cross_entropy)}')\n",
    "print(f'Mean IoU: {np.mean(iou_score)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cfg['NUM_CLASSES']):\n",
    "    print('*******************')\n",
    "    print(f'***** Class {i} *****')\n",
    "    print('*******************')\n",
    "    print(f'Mean dice coeff: {np.mean(dice_coeff[:, i])}')\n",
    "    print(f'Mean dice coeff (with mask): {np.mean(dice_coeff[:, i][mask_not_empty[:, i]])}')\n",
    "    print(f'Mean dice coeff (no mask): {np.mean(dice_coeff[:, i][~mask_not_empty[:, i]])}')\n",
    "    print(f'Mean IoU: {np.mean(iou_score[:, i])}')\n",
    "    print(f'Mean IoU (with mask): {np.mean(iou_score[:, i][mask_not_empty[:, i]])}')\n",
    "    print(f'Mean IoU (no mask): {np.mean(iou_score[:, i][~mask_not_empty[:, i]])}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8\n",
    "*******************\n",
    "***** Class 0 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.8249650597572327\n",
    "Mean dice coeff (with mask): 0.5211997032165527\n",
    "Mean dice coeff (no mask): 0.85056471824646\n",
    "Mean IoU: 0.813450276851654\n",
    "Mean IoU (with mask): 0.3730500340461731\n",
    "Mean IoU (no mask): 0.85056471824646\n",
    "\n",
    "*******************\n",
    "***** Class 1 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.9306501746177673\n",
    "Mean dice coeff (with mask): 0.5132780075073242\n",
    "Mean dice coeff (no mask): 0.9423393607139587\n",
    "Mean IoU: 0.9266247153282166\n",
    "Mean IoU (with mask): 0.365519642829895\n",
    "Mean IoU (no mask): 0.9423393607139587\n",
    "\n",
    "*******************\n",
    "***** Class 2 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.706938624382019\n",
    "Mean dice coeff (with mask): 0.6717405915260315\n",
    "Mean dice coeff (no mask): 0.7313432693481445\n",
    "Mean IoU: 0.651603102684021\n",
    "Mean IoU (with mask): 0.5365963578224182\n",
    "Mean IoU (no mask): 0.7313432693481445\n",
    "\n",
    "*******************\n",
    "***** Class 3 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.9221725463867188\n",
    "Mean dice coeff (with mask): 0.7034797668457031\n",
    "Mean dice coeff (no mask): 0.9367521405220032\n",
    "Mean IoU: 0.9135252833366394\n",
    "Mean IoU (with mask): 0.5651221871376038\n",
    "Mean IoU (no mask): 0.9367521405220032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7\n",
    "*******************\n",
    "***** Class 0 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.7857795357704163\n",
    "Mean dice coeff (with mask): 0.5015755891799927\n",
    "Mean dice coeff (no mask): 0.8097306489944458\n",
    "Mean IoU: 0.7742512822151184\n",
    "Mean IoU (with mask): 0.3532538414001465\n",
    "Mean IoU (no mask): 0.8097306489944458\n",
    "\n",
    "*******************\n",
    "***** Class 1 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.9162784218788147\n",
    "Mean dice coeff (with mask): 0.485749751329422\n",
    "Mean dice coeff (no mask): 0.9283360838890076\n",
    "Mean IoU: 0.9122956991195679\n",
    "Mean IoU (with mask): 0.3395600914955139\n",
    "Mean IoU (no mask): 0.9283360838890076\n",
    "\n",
    "*******************\n",
    "***** Class 2 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.6629075407981873\n",
    "Mean dice coeff (with mask): 0.6581381559371948\n",
    "Mean dice coeff (no mask): 0.6662144064903259\n",
    "Mean IoU: 0.6058319211006165\n",
    "Mean IoU (with mask): 0.5187441110610962\n",
    "Mean IoU (no mask): 0.6662144064903259\n",
    "\n",
    "*******************\n",
    "***** Class 3 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.9067474603652954\n",
    "Mean dice coeff (with mask): 0.7002676129341125\n",
    "Mean dice coeff (no mask): 0.9205127954483032\n",
    "Mean IoU: 0.8978906273841858\n",
    "Mean IoU (with mask): 0.5585583448410034\n",
    "Mean IoU (no mask): 0.9205127954483032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6\n",
    "*******************\n",
    "***** Class 0 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.7556981444358826\n",
    "Mean dice coeff (with mask): 0.4856829345226288\n",
    "Mean dice coeff (no mask): 0.778453528881073\n",
    "Mean IoU: 0.7441787719726562\n",
    "Mean IoU (with mask): 0.3374749422073364\n",
    "Mean IoU (no mask): 0.778453528881073\n",
    "\n",
    "*******************\n",
    "***** Class 1 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.903589129447937\n",
    "Mean dice coeff (with mask): 0.4611539840698242\n",
    "Mean dice coeff (no mask): 0.9159802198410034\n",
    "Mean IoU: 0.8996537327766418\n",
    "Mean IoU (with mask): 0.31670063734054565\n",
    "Mean IoU (no mask): 0.9159802198410034\n",
    "\n",
    "*******************\n",
    "***** Class 2 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.6304072141647339\n",
    "Mean dice coeff (with mask): 0.6433426737785339\n",
    "Mean dice coeff (no mask): 0.6214382648468018\n",
    "Mean IoU: 0.571916937828064\n",
    "Mean IoU (with mask): 0.5004937648773193\n",
    "Mean IoU (no mask): 0.6214382648468018\n",
    "\n",
    "*******************\n",
    "***** Class 3 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.8894842863082886\n",
    "Mean dice coeff (with mask): 0.6932864785194397\n",
    "Mean dice coeff (no mask): 0.9025641083717346\n",
    "Mean IoU: 0.8804334998130798\n",
    "Mean IoU (with mask): 0.5484738349914551\n",
    "Mean IoU (no mask): 0.9025641083717346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5\n",
    "*******************\n",
    "***** Class 0 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.7198657393455505\n",
    "Mean dice coeff (with mask): 0.46796393394470215\n",
    "Mean dice coeff (no mask): 0.741094708442688\n",
    "Mean IoU: 0.7083962559700012\n",
    "Mean IoU (with mask): 0.32039639353752136\n",
    "Mean IoU (no mask): 0.741094708442688\n",
    "\n",
    "*******************\n",
    "***** Class 1 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.895077645778656\n",
    "Mean dice coeff (with mask): 0.4428459107875824\n",
    "Mean dice coeff (no mask): 0.907742977142334\n",
    "Mean IoU: 0.8911918997764587\n",
    "Mean IoU (with mask): 0.3002220690250397\n",
    "Mean IoU (no mask): 0.907742977142334\n",
    "\n",
    "*******************\n",
    "***** Class 2 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.5963675379753113\n",
    "Mean dice coeff (with mask): 0.6287020444869995\n",
    "Mean dice coeff (no mask): 0.5739484429359436\n",
    "Mean IoU: 0.5370190143585205\n",
    "Mean IoU (with mask): 0.4837566316127777\n",
    "Mean IoU (no mask): 0.5739484429359436\n",
    "\n",
    "*******************\n",
    "***** Class 3 *****\n",
    "*******************\n",
    "Mean dice coeff: 0.8759523034095764\n",
    "Mean dice coeff (with mask): 0.6819044947624207\n",
    "Mean dice coeff (no mask): 0.8888888955116272\n",
    "Mean IoU: 0.8667181730270386\n",
    "Mean IoU (with mask): 0.5341587662696838\n",
    "Mean IoU (no mask): 0.8888888955116272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = None\n",
    "mask_only = True\n",
    "\n",
    "scores = None\n",
    "if class_id is None:\n",
    "    print('Worst scores for class all classes:')\n",
    "    scores = np.mean(dice_coeff, axis=-1)\n",
    "else:\n",
    "    print(f'Worst scores for class {class_id}')\n",
    "    scores = dice_coeff[:, class_id]\n",
    "\n",
    "indices = np.argsort(scores) # Indices of worst images\n",
    "\n",
    "if mask_only and class_id is not None:\n",
    "    print('Including scores for non-empty ground truth masks only.')\n",
    "    mask_only_indices = np.where(mask_not_empty[:, class_id])\n",
    "    mask_only_indices = set(mask_only_indices[0].tolist())\n",
    "    indices = [index for index in indices if index in mask_only_indices]\n",
    "\n",
    "for i in indices:\n",
    "    print(f'{i}: {scores[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Image Prediction\n",
    "img_id = 423\n",
    "thresh = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "img_name = val_imgs[img_id]\n",
    "img, ann = dataset.get_example_from_img_name(img_name)\n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "y = seg_model.predict(img_batch)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(visualize_segmentations(np.repeat(img, 3, axis=-1), ann))\n",
    "plt.show()\n",
    "\n",
    "for i in range(y.shape[-1]):\n",
    "    plt.figure(figsize=(12.5, 3))\n",
    "    plt.imshow(y[0, :, :, i])\n",
    "    plt.colorbar()\n",
    "\n",
    "for i in range(y.shape[-1]):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(y[0, :, :, i] > thresh[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save HDF5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "seg_model.save(f'seg_model_{date_str}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
