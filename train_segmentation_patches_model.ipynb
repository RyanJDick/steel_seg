{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a UNet segmentation model on the Severstal steel defect dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from steel_seg.utils import (\n",
    "    dice_coeff_kaggle,\n",
    "    per_class_dice_coeff,\n",
    "    rle_to_dense,\n",
    "    dense_to_rle,\n",
    "    visualize_segmentations,\n",
    "    onehottify)\n",
    "from steel_seg.dataset.severstal_steel_dataset import SeverstalSteelDataset\n",
    "from steel_seg.dataset.severstal_steel_dataset_patch_generator import \\\n",
    "    SeverstalSteelDatasetPatchGenerator\n",
    "from steel_seg.model.unet import build_unet_model\n",
    "from steel_seg.model.deep_q_postprocessor import build_deep_q_model\n",
    "from steel_seg.train import (\n",
    "    class_weighted_binary_crossentropy,\n",
    "    weighted_binary_crossentropy,\n",
    "    pixel_map_weighted_binary_crossentropy,\n",
    "    dice_loss_multi_class,\n",
    "    dice_coef,\n",
    "    DiceCoefByClassAndEmptiness,\n",
    "    eval)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just load this dataset so we can use the same train/val split\n",
    "dataset = SeverstalSteelDataset.init_from_config('SETTINGS.yaml')\n",
    "train_imgs = dataset.get_image_list('training')\n",
    "val_imgs = dataset.get_image_list('validation')\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = SeverstalSteelDatasetPatchGenerator.init_from_config(\n",
    "    'SETTINGS.yaml',\n",
    "    train_imgs,\n",
    "    is_training=True)\n",
    "val_data_gen = SeverstalSteelDatasetPatchGenerator.init_from_config(\n",
    "    'SETTINGS.yaml',\n",
    "    val_imgs,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = build_unet_model(\n",
    "    img_height=cfg['PATCH_SIZE'],\n",
    "    img_width=cfg['PATCH_SIZE'],\n",
    "    img_channels=1,\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 2),\n",
    "    num_features=[32, 64, 128, 256],\n",
    "    drop_prob=0.5)\n",
    "model_checkpoint_name = 'patches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_imgs = dataset.get_image_list('training')\n",
    "\n",
    "# cls_pixel_counts = np.array([0, 0, 0, 0], dtype=np.float64)\n",
    "# total_pixels = 0.0\n",
    "# for img_name in train_imgs:\n",
    "#     img, ann = dataset.get_example_from_img_name(img_name)\n",
    "#     img_pixel_counts = np.sum(ann, axis=(0, 1))\n",
    "#     cls_pixel_counts += img_pixel_counts\n",
    "#     total_pixels += ann.size\n",
    "# cls_pixel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_weights = total_pixels / cls_pixel_counts\n",
    "# cls_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_weights = [5274.42494602, 26340.24368548, 158.51888478, 752.96782738]\n",
    "#cls_weights = np.array([5274.42494602, 26340.24368548, 158.51888478, 752.96782738]) / 20\n",
    "#cls_weights = [263.7212473, 1317.01218427, 7.92594424, 37.64839137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_2_weight = 10.0\n",
    "# cls_0_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[0]\n",
    "# cls_1_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[1]\n",
    "# cls_3_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[3]\n",
    "# cls_weights = [cls_0_weight, cls_1_weight, cls_2_weight, cls_3_weight]\n",
    "#cls_weights = [332.7316460371491, 1661.6470474376874, 10.0, 47.50019711767978]\n",
    "#cls_weights = [30.0, 40.0, 10.0, 20.0]\n",
    "#cls_weights = [40.0, 50.0, 5.0, 20.0]\n",
    "cls_weights = [2.0, 4.0, 1.0, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.0001),\n",
    "    loss=pixel_map_weighted_binary_crossentropy(cls_weights),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(),\n",
    "        dice_coef(batch_size=cfg['BATCH_SIZE']),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=0, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls0'),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=1, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls1'),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=2, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls2'),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=3, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls3'),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=0, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls0'),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=1, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls1'),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=2, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls2'),\n",
    "        #DiceCoefByClassAndEmptiness(cls_id=3, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls3'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Initial Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = None\n",
    "date_str = '20190928-210154'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('No checkpoints found. Starting from scratch.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    seg_model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use new model name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_name = 'patches_lower_weight'\n",
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#initial_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss', #'val__dice_coef'\n",
    "    save_best_only=True,\n",
    "    mode='auto',#'auto', 'max',\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "logdir = f'logs/{checkpoint_name}-{initial_epoch}'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "    checkpoint_cb,\n",
    "]\n",
    "\n",
    "results = seg_model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=400,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data_gen,\n",
    "    steps_per_epoch=len(train_data_gen),\n",
    "    validation_steps=len(val_data_gen),\n",
    "    validation_freq=3,\n",
    "    initial_epoch=initial_epoch,\n",
    "    workers=6,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steel_seg.dataset.severstal_steel_dataset_patch_generator import get_image_patches\n",
    "\n",
    "def per_class_binary_cross_entropy(y_pred, y_true):\n",
    "    assert len(y_true.shape) == 3\n",
    "    eps = 0.000001\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    per_pixel_class_cross_entropy = \\\n",
    "        y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "    per_class_cross_entropy = np.mean(per_pixel_class_cross_entropy, axis=(0, 1))\n",
    "    return per_class_cross_entropy\n",
    "\n",
    "def per_class_iou_score(y_pred, y_true):\n",
    "    assert len(y_true.shape) == 3\n",
    "    intersection = np.sum(y_pred * y_true, axis=(0, 1))\n",
    "    union = np.sum(y_pred, axis=(0, 1)) + np.sum(y_true, axis=(0, 1)) - intersection\n",
    "    \n",
    "    iou = []\n",
    "    for i in range(intersection.shape[0]):\n",
    "        if union[i] == 0:\n",
    "            iou.append(1.0) # Both y_pred and y_true were empty\n",
    "        else:\n",
    "            iou.append(intersection[i] / union[i])\n",
    "    return np.array(iou)\n",
    "\n",
    "def per_class_mask_not_empty(y_true):\n",
    "    return np.sum(y_true, axis=(0, 1)) > 0.5\n",
    "\n",
    "def predict_patches(model, img, patch_size, num_patches_per_image, num_classes):\n",
    "    h, w, c = img.shape\n",
    "    img_patches, x_step_size = get_image_patches(img, patch_size, num_patches_per_image)\n",
    "    img_patches = np.stack(img_patches)\n",
    "    y_patches = model.predict(img_patches)\n",
    "    \n",
    "    num_patches = y_patches.shape[0]\n",
    "    combined_patches = np.zeros((num_patches, h, w, num_classes), dtype=np.float32)\n",
    "    for i in range(num_patches):\n",
    "        x_start = i * x_step_size\n",
    "        combined_patches[i, :, x_start:x_start+patch_size, :] = y_patches[i, :, :, :]\n",
    "    y = np.amax(combined_patches, axis=0, keepdims=True)\n",
    "    return y\n",
    "\n",
    "def eval_segmentation(\n",
    "    model,\n",
    "    dataset,\n",
    "    img_list,\n",
    "    patch_size,\n",
    "    num_patches_per_image,\n",
    "    thresholds=None,\n",
    "    num_classes=4):\n",
    "    \n",
    "    binary_cross_entropy = np.zeros((len(img_list), num_classes), dtype=np.float32)\n",
    "    dice_coeff = np.zeros((len(img_list), num_classes), dtype=np.float32)\n",
    "    iou_score = np.zeros((len(img_list), num_classes), dtype=np.float32)\n",
    "    mask_not_empty = np.zeros((len(img_list), num_classes), dtype=np.bool)\n",
    "    \n",
    "    if thresholds is None:\n",
    "        thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "    thresholds = np.array(thresholds)\n",
    "\n",
    "    for i, img_name in enumerate(img_list):\n",
    "        img, ann = dataset.get_example_from_img_name(img_name)\n",
    "        y = predict_patches(model, img, patch_size, num_patches_per_image, num_classes)\n",
    "        \n",
    "        # Binarize predictions\n",
    "        y_bin = np.zeros_like(y, dtype=np.uint8)\n",
    "        y_bin[y > thresholds] = 1\n",
    "\n",
    "        binary_cross_entropy[i, :] = per_class_binary_cross_entropy(y[0, :, :, :], ann)\n",
    "        dice_coeff[i, :] = per_class_dice_coeff(y_bin[0, :, :, :], ann)\n",
    "        iou_score[i, :] = per_class_iou_score(y_bin[0, :, :, :], ann)\n",
    "        mask_not_empty[i, :] = per_class_mask_not_empty(ann)\n",
    "        \n",
    "    return binary_cross_entropy, dice_coeff, iou_score, mask_not_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy, dice_coeff, iou_score, mask_not_empty = eval_segmentation(\n",
    "    seg_model,\n",
    "    val_data_gen,\n",
    "    val_imgs,\n",
    "    patch_size=cfg['PATCH_SIZE'],\n",
    "    num_patches_per_image=cfg['NUM_PATCHES_PER_IMAGE_VAL'],\n",
    "    thresholds=None,\n",
    "    num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean dice coeff: {np.mean(dice_coeff)}')\n",
    "print(f'Mean binary cross entropy: {np.mean(binary_cross_entropy)}')\n",
    "print(f'Mean IoU: {np.mean(iou_score)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cfg['NUM_CLASSES']):\n",
    "    print('*******************')\n",
    "    print(f'***** Class {i} *****')\n",
    "    print('*******************')\n",
    "    print(f'Mean dice coeff: {np.mean(dice_coeff[:, i])}')\n",
    "    print(f'Mean dice coeff (with mask): {np.mean(dice_coeff[:, i][mask_not_empty[:, i]])}')\n",
    "    print(f'Mean dice coeff (no mask): {np.mean(dice_coeff[:, i][~mask_not_empty[:, i]])}')\n",
    "    print(f'Mean IoU: {np.mean(iou_score[:, i])}')\n",
    "    print(f'Mean IoU (with mask): {np.mean(iou_score[:, i][mask_not_empty[:, i]])}')\n",
    "    print(f'Mean IoU (no mask): {np.mean(iou_score[:, i][~mask_not_empty[:, i]])}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = None\n",
    "mask_only = True\n",
    "\n",
    "scores = None\n",
    "if class_id is None:\n",
    "    print('Worst scores for class all classes:')\n",
    "    scores = np.mean(dice_coeff, axis=-1)\n",
    "else:\n",
    "    print(f'Worst scores for class {class_id}')\n",
    "    scores = dice_coeff[:, class_id]\n",
    "\n",
    "indices = np.argsort(scores) # Indices of worst images\n",
    "\n",
    "if mask_only and class_id is not None:\n",
    "    print('Including scores for non-empty ground truth masks only.')\n",
    "    mask_only_indices = np.where(mask_not_empty[:, class_id])\n",
    "    mask_only_indices = set(mask_only_indices[0].tolist())\n",
    "    indices = [index for index in indices if index in mask_only_indices]\n",
    "\n",
    "for i in indices:\n",
    "    print(f'{i}: {scores[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Image Prediction\n",
    "img_id = 872\n",
    "thresh = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "img_name = val_imgs[img_id]\n",
    "img, ann = val_data_gen.get_example_from_img_name(img_name)\n",
    "y = predict_patches(seg_model, img, cfg['PATCH_SIZE'], cfg['NUM_PATCHES_PER_IMAGE_VAL'], cfg['NUM_CLASSES'])\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(visualize_segmentations(np.repeat(img, 3, axis=-1), ann))\n",
    "plt.show()\n",
    "\n",
    "for i in range(y.shape[-1]):\n",
    "    plt.figure(figsize=(12.5, 3))\n",
    "    plt.imshow(y[0, :, :, i])\n",
    "    plt.colorbar()\n",
    "\n",
    "for i in range(y.shape[-1]):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(y[0, :, :, i] > thresh[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save HDF5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "seg_model.save(f'patches_seg_model_{date_str}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
