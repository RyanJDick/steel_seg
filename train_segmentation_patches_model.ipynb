{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a UNet segmentation model on the Severstal steel defect dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from steel_seg.utils import (\n",
    "    dice_coeff_kaggle,\n",
    "    rle_to_dense,\n",
    "    dense_to_rle,\n",
    "    visualize_segmentations,\n",
    "    onehottify)\n",
    "from steel_seg.dataset.severstal_steel_dataset import SeverstalSteelDataset\n",
    "from steel_seg.model.unet import build_unet_model\n",
    "from steel_seg.model.deep_q_postprocessor import build_deep_q_model\n",
    "from steel_seg.train import (\n",
    "    class_weighted_binary_crossentropy,\n",
    "    weighted_binary_crossentropy,\n",
    "    pixel_map_weighted_binary_crossentropy,\n",
    "    dice_loss_multi_class,\n",
    "    dice_coef,\n",
    "    DiceCoefByClassAndEmptiness,\n",
    "    eval)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeverstalSteelDataset.init_from_config('SETTINGS.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_batches = dataset.create_dataset(dataset_type='training', use_patches=True)\n",
    "val_data, val_batches = dataset.create_dataset(dataset_type='validation', use_patches=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = build_unet_model(\n",
    "    img_height=cfg['PATCH_SIZE'],\n",
    "    img_width=cfg['PATCH_SIZE'],\n",
    "    img_channels=1,\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 2),\n",
    "    num_features=[32, 64, 128, 256],\n",
    "    drop_prob=0.5)\n",
    "model_checkpoint_name = 'patches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_imgs = dataset.get_image_list('training')\n",
    "\n",
    "# cls_pixel_counts = np.array([0, 0, 0, 0], dtype=np.float64)\n",
    "# total_pixels = 0.0\n",
    "# for img_name in train_imgs:\n",
    "#     img, ann = dataset.get_example_from_img_name(img_name)\n",
    "#     img_pixel_counts = np.sum(ann, axis=(0, 1))\n",
    "#     cls_pixel_counts += img_pixel_counts\n",
    "#     total_pixels += ann.size\n",
    "# cls_pixel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_weights = total_pixels / cls_pixel_counts\n",
    "# cls_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_weights = [5274.42494602, 26340.24368548, 158.51888478, 752.96782738]\n",
    "#cls_weights = np.array([5274.42494602, 26340.24368548, 158.51888478, 752.96782738]) / 20\n",
    "#cls_weights = [263.7212473, 1317.01218427, 7.92594424, 37.64839137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_2_weight = 10.0\n",
    "# cls_0_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[0]\n",
    "# cls_1_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[1]\n",
    "# cls_3_weight = (cls_pixel_counts[2] * cls_2_weight) / cls_pixel_counts[3]\n",
    "# cls_weights = [cls_0_weight, cls_1_weight, cls_2_weight, cls_3_weight]\n",
    "#cls_weights = [332.7316460371491, 1661.6470474376874, 10.0, 47.50019711767978]\n",
    "#cls_weights = [30.0, 40.0, 10.0, 20.0]\n",
    "cls_weights = [40.0, 50.0, 5.0, 20.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.0001),\n",
    "    loss=pixel_map_weighted_binary_crossentropy(cls_weights),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(),\n",
    "        dice_coef(batch_size=cfg['BATCH_SIZE']),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=0, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls0'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=1, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls1'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=2, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls2'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=3, empty_masks_only=True, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_empty_cls3'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=0, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls0'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=1, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls1'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=2, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls2'),\n",
    "        DiceCoefByClassAndEmptiness(cls_id=3, empty_masks_only=False, batch_size=cfg['BATCH_SIZE'], name='dice_coef_on_non_empty_cls3'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_str = '20190916-092052'\n",
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('No checkpoints found. Starting from scratch.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    seg_model.load_weights(latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val__dice_coef',#'val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='max',#'auto',\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "logdir = f'logs/{checkpoint_name}-{initial_epoch}'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "    checkpoint_cb,\n",
    "]\n",
    "\n",
    "results = seg_model.fit(train_data,\n",
    "                    epochs=400,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_data,\n",
    "                    steps_per_epoch=train_batches,\n",
    "                    validation_steps=val_batches,\n",
    "                    validation_freq=3,\n",
    "                    initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = dataset.get_image_list('validation')\n",
    "len(val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Move these functions out to .py files\n",
    "\n",
    "# postprocess(y, thresh=0.8, upper_thresh=0.8, num_px_thresh=10000): 0.8795549804140013\n",
    "# postprocess(y, thresh=0.7, upper_thresh=0.7, num_px_thresh=10000): 0.8685514979767833\n",
    "# postprocess(y, thresh=0.8, upper_thresh=0.8, num_px_thresh=20000): 0.8597784574292987\n",
    "# postprocess(y, thresh=0.8, upper_thresh=0.8, num_px_thresh=5000):  0.888272717085689\n",
    "# postprocess(y, thresh=0.85, upper_thresh=0.85, num_px_thresh=5000): 0.8933982786371334\n",
    "# postprocess(y, thresh=0.85, upper_thresh=0.85, num_px_thresh=2000): 0.8862070712564566\n",
    "# postprocess(y, thresh=0.9, upper_thresh=0.9, num_px_thresh=2000):  0.8899766923286324\n",
    "\n",
    "def postprocess(y, thresh=None, upper_thresh=None, num_px_thresh=None):\n",
    "    if thresh is None:\n",
    "        thresh = [0.85, 0.85, 0.85, 0.85]\n",
    "    if upper_thresh is None:\n",
    "        upper_thresh = [0.85, 0.85, 0.85, 0.85]\n",
    "    if num_px_thresh is None:\n",
    "        num_px_thresh = [5000, 5000, 5000, 5000]\n",
    "    # TODO: handle batches properly\n",
    "    batches, height, width, classes = y.shape\n",
    "    assert batches == 1\n",
    "    \n",
    "    # Only allow one class at each pixel\n",
    "    y_argmax = np.argmax(y, axis=-1)\n",
    "    y_one_hot = onehottify(y_argmax, y.shape[-1], dtype=int)\n",
    "    for c in range(classes):\n",
    "        y_one_hot[:, :, :, c][y[:, :, :, c] < thresh[c]] = 0 # Background\n",
    "    \n",
    "    # Making predictions on an empty mask is very costly (score immediately goes from 1 to 0)\n",
    "    # So, only predict a mask if there are many pixels (num_px_thresh) above a high threshold (upper_thresh)\n",
    "    for c in range(classes):\n",
    "        pixels_above_upper = np.sum(y[:, :, :, c] > upper_thresh[c])\n",
    "        if pixels_above_upper < num_px_thresh[c]:\n",
    "            y_one_hot[:, :, :, c] = 0\n",
    "    return y_one_hot\n",
    "    \n",
    "#     # Segmentations seem to be drawn with a tool that has a minimum width of ~20 pixels\n",
    "#     min_width_div_2 = 10\n",
    "#     for c in range(classes):\n",
    "#         for row in range(height):\n",
    "#             mask_start = None\n",
    "#             for col in range(width):\n",
    "#                 if y_one_hot[0, row, col, c]: # Pixel is part of mask\n",
    "#                     if mask_start is None: # First pixel in mask from left to right\n",
    "#                         mask_start = col\n",
    "#                 else: # Pixel is not part of mask\n",
    "#                     if mask_start is not None: # End of mask from left to right\n",
    "#                         mask_mid = int((col - mask_start) / 2) + col\n",
    "#                         new_start = max(0, mask_mid - min_width_div_2)\n",
    "#                         new_end = min(y_one_hot.shape[2], mask_mid + min_width_div_2)\n",
    "#                         y_one_hot[0, row, new_start:new_end, c] = 1\n",
    "#                     mask_start = None\n",
    "#             # I'm lazy, so don't bother dealing with masks that go off the right edge\n",
    "\n",
    "def eval(model, dataset, img_list, num_classes=4, thresh=None, upper_thresh=None, num_px_thresh=None, verbose=False):\n",
    "    num_empty_gt = [0] * num_classes\n",
    "    num_empty_gt_mask_pred = [0] * num_classes\n",
    "    num_mask_gt_empty_pred = [0] * num_classes\n",
    "    mask_sizes = [[] for _ in range(num_classes)]\n",
    "\n",
    "    dice_coeffs = []\n",
    "    for img_name in img_list:\n",
    "        img, ann = dataset.get_example_from_img_name(img_name)\n",
    "        img_batch = np.expand_dims(img, axis=0)\n",
    "        y = model.predict(img_batch)\n",
    "        y_one_hot = postprocess(y, thresh=thresh, upper_thresh=upper_thresh, num_px_thresh=num_px_thresh)\n",
    "        dice_coeffs.append(dice_coeff_kaggle(y_one_hot[0, :, :, :], ann))\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            gt_mask_size = np.count_nonzero(ann[:, :, c])\n",
    "            gt_is_empty = gt_mask_size == 0\n",
    "            pred_is_empty = np.count_nonzero(y_one_hot[0, :, :, c]) == 0\n",
    "            \n",
    "            if gt_is_empty:\n",
    "                num_empty_gt[c] += 1\n",
    "            else:\n",
    "                mask_sizes[c].append(gt_mask_size)\n",
    "\n",
    "            if gt_is_empty and not pred_is_empty:\n",
    "                num_empty_gt_mask_pred[c] += 1\n",
    "            \n",
    "            if not gt_is_empty and pred_is_empty:\n",
    "                num_mask_gt_empty_pred[c] += 1\n",
    "\n",
    "    if verbose:\n",
    "        for c in range(num_classes):\n",
    "            print(f'**** Class {c} ****')\n",
    "            print(f'Num empty gt masks: {num_empty_gt[c]}')\n",
    "            print(f'Num non-empty gt masks: {len(img_list) - num_empty_gt[c]}')\n",
    "            print(f'Num empty gt mask and non-empty pred: {num_empty_gt_mask_pred[c]} '\n",
    "                  f'({num_empty_gt_mask_pred[c] / num_empty_gt[c]})')\n",
    "            print(f'Num non-empty gt mask and empty pred: {num_mask_gt_empty_pred[c]} '\n",
    "                  f'({num_mask_gt_empty_pred[c] / (len(img_list) - num_empty_gt[c])})')\n",
    "            print(f'Mean mask size: {np.mean(mask_sizes[c])} (stddev: {np.std(mask_sizes[c])})')\n",
    "        \n",
    "    mean_dice_coeff = np.mean(dice_coeffs)\n",
    "    print(f'Mean dice coeff: {mean_dice_coeff}')\n",
    "    return mean_dice_coeff, dice_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = [0.5, 0.5, 0.5, 0.5]\n",
    "upper_thresh = [0.85, 0.85, 0.85, 0.85]\n",
    "num_px_thresh = [5000, 5000, 5000, 5000]\n",
    "\n",
    "mean_dice_coeff, dice_coeffs = eval(\n",
    "    seg_model,\n",
    "    dataset,\n",
    "    val_imgs,\n",
    "    thresh=thresh,\n",
    "    upper_thresh=upper_thresh,\n",
    "    num_px_thresh=num_px_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search for each class:\n",
    "import threading\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "thresh_grid = [0.8, 0.9, 0.95]\n",
    "upper_thresh_grid = [0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "num_px_thresh_grid = [2000, 5000, 10000, 20000]\n",
    "\n",
    "grid_size = len(thresh_grid) * len(upper_thresh_grid) * len(num_px_thresh_grid)\n",
    "grid_search_dice_scores = []\n",
    "for i in range(4):\n",
    "    grid_search_dice_scores.append([])\n",
    "    for _ in range(grid_size):\n",
    "        grid_search_dice_scores[i].append([])\n",
    "\n",
    "def postprocess_worker(thresh, upper_thresh, num_px_thresh, y, ann, grid_search_dice_scores, index):\n",
    "    y_one_hot = postprocess(y,\n",
    "                            thresh=thresh,\n",
    "                            upper_thresh=upper_thresh,\n",
    "                            num_px_thresh=num_px_thresh)\n",
    "    for cls in range(y_one_hot.shape[-1]):\n",
    "        grid_search_dice_scores[cls][index].append(\n",
    "            dice_coeff_kaggle(y_one_hot[0, :, :, cls:cls+1], ann[:, :, cls:cls+1]))\n",
    "\n",
    "for i, img_name in enumerate(val_imgs):\n",
    "    if i % 10 == 0:\n",
    "        print(f'Processing image {i + 1} / {len(val_imgs)}')\n",
    "\n",
    "    img, ann = dataset.get_example_from_img_name(img_name)\n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    y = seg_model.predict(img_batch)\n",
    "    \n",
    "    index = 0\n",
    "    threads = []\n",
    "    for thresh, upper_thresh, num_px_thresh in product(thresh_grid, upper_thresh_grid, num_px_thresh_grid):\n",
    "        thresh_array = [thresh, thresh, thresh, thresh]\n",
    "        upper_thresh_array = [upper_thresh, upper_thresh, upper_thresh, upper_thresh]\n",
    "        num_pc_thresh_array = [num_px_thresh, num_px_thresh, num_px_thresh, num_px_thresh]\n",
    "        \n",
    "        t = threading.Thread(\n",
    "            target=postprocess_worker,\n",
    "            args=(thresh_array, upper_thresh_array, num_pc_thresh_array, y, ann, grid_search_dice_scores, index)\n",
    "        )\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "#         y_one_hot = postprocess(y,\n",
    "#                                 thresh=[thresh, thresh, thresh, thresh],\n",
    "#                                 upper_thresh=[upper_thresh, upper_thresh, upper_thresh, upper_thresh],\n",
    "#                                 num_px_thresh=[num_px_thresh, num_px_thresh, num_px_thresh, num_px_thresh])\n",
    "#         for cls in range(y_one_hot.shape[-1]):\n",
    "#             grid_search_dice_scores[cls][index].append(dice_coeff_kaggle(y_one_hot[0, :, :, cls:cls+1], ann[:, :, cls:cls+1]))\n",
    "        index += 1\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "for cls in range(4):\n",
    "    print('********************')\n",
    "    print(f'Results for class {cls}:')\n",
    "    print('********************')\n",
    "    cls_scores = grid_search_dice_scores[cls]\n",
    "    index = 0\n",
    "    max_score = 0\n",
    "    max_score_str = ''\n",
    "    for thresh, upper_thresh, num_px_thresh in product(thresh_grid, upper_thresh_grid, num_px_thresh_grid):\n",
    "        total_score = np.mean(cls_scores[index])\n",
    "        desc_str = f'thresh={thresh}, upper_thresh={upper_thresh}, num_px_thresh={num_px_thresh}:\\t{total_score}'\n",
    "        if total_score > max_score:\n",
    "            max_score = total_score\n",
    "            max_score_str = desc_str\n",
    "        print(desc_str)\n",
    "        index += 1\n",
    "    print('Best hyperparam combination:')\n",
    "    print(max_score_str + '\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class 0: thresh=0.9, upper_thresh=0.7, num_px_thresh=5000:\t0.9281188718499144\n",
    "Class 1: thresh=0.9, upper_thresh=0.7, num_px_thresh=2000:\t0.9878215327643398\n",
    "Class 2: thresh=0.8, upper_thresh=0.9, num_px_thresh=2000:\t0.8120156858142843\n",
    "Class 3: thresh=0.8, upper_thresh=0.9, num_px_thresh=5000:\t0.9748260642241272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class 0: thresh=0.95, upper_thresh=0.7, num_px_thresh=5000:\t0.9359843396575916\n",
    "Class 1: thresh=0.9, upper_thresh=0.7, num_px_thresh=2000:\t0.9878215327643398\n",
    "Class 2: thresh=0.9, upper_thresh=0.95, num_px_thresh=2000:\t0.8149881087479656\n",
    "Class 3: thresh=0.8, upper_thresh=0.95, num_px_thresh=2000:\t0.9749012164048724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean dice coeff: 0.9140845065637913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "HP_SPACE = [\n",
    "    skopt.space.Real(0.0, 1.0, name='cls1_thresh'),\n",
    "    skopt.space.Real(0.0, 1.0, name='cls2_thresh'),\n",
    "    skopt.space.Real(0.0, 1.0, name='cls3_thresh'),\n",
    "    skopt.space.Real(0.0, 1.0, name='cls4_thresh'),\n",
    "    skopt.space.Real(0.0, 1.0, name='cls1_upper_thresh'),\n",
    "    skopt.space.Real(0.0, 1.0, name='cls2_upper_thresh'),\n",
    "    skopt.space.Real(0.0, 1.0, name='cls3_upper_thresh'),\n",
    "    skopt.space.Real(0.0, 1.0, name='cls4_upper_thresh'),\n",
    "    skopt.space.Integer(0, 50000, name='cls1_px_thresh'),\n",
    "    skopt.space.Integer(0, 50000, name='cls2_px_thresh'),\n",
    "    skopt.space.Integer(0, 50000, name='cls3_px_thresh'),\n",
    "    skopt.space.Integer(0, 50000, name='cls4_px_thresh'),\n",
    "]\n",
    "\n",
    "def build_eval_postprocess_hyperparams(model, dataset, val_imgs):\n",
    "    @skopt.utils.use_named_args(HP_SPACE)\n",
    "    def eval_postprocess_hyperparams(**kwargs):\n",
    "        thresh = [kwargs['cls1_thresh'], kwargs['cls2_thresh'], kwargs['cls3_thresh'], kwargs['cls4_thresh']]\n",
    "        upper_thresh = [kwargs['cls1_upper_thresh'], kwargs['cls2_upper_thresh'], kwargs['cls3_upper_thresh'], kwargs['cls4_upper_thresh']]\n",
    "        num_px_thresh = [kwargs['cls1_px_thresh'], kwargs['cls2_px_thresh'], kwargs['cls3_px_thresh'], kwargs['cls4_px_thresh']]\n",
    "\n",
    "        score, _ = eval(model, dataset, val_imgs, thresh=thresh, upper_thresh=upper_thresh, num_px_thresh=num_px_thresh)\n",
    "        return 1 - score\n",
    "    return eval_postprocess_hyperparams\n",
    "\n",
    "res = skopt.gbrt_minimize(\n",
    "    build_eval_postprocess_hyperparams(seg_model, dataset, val_imgs),\n",
    "    HP_SPACE,\n",
    "    n_calls=120,\n",
    "    n_random_starts=10,\n",
    "    random_state=123,\n",
    "    verbose=True,\n",
    "    n_jobs=6)\n",
    "# res = skopt.gp_minimize(\n",
    "#     build_eval_postprocess_hyperparams(model, dataset, val_imgs), # the function to minimize\n",
    "#     HP_SPACE,                                                     # the bounds on each dimension of x\n",
    "#     acq_func=\"EI\",                                                # the acquisition function\n",
    "#     n_calls=120,                                                  # the number of evaluations of f \n",
    "#     n_random_starts=20,                                           # the number of random initialization points\n",
    "#     random_state=123)                                             # the random seed\n",
    "\n",
    "#skopt.dump(res, 'skopt_result.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = [0.989482241473342, 0.4681867574192067, 0.8408261195643344, 0.7391752127314548]\n",
    "upper_thresh = [0.02055732831801516, 0.8815747640346393, 0.8895357807949745, 0.84233544712227]\n",
    "num_px_thresh = [17148, 29846, 5021, 34434]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skopt.dump(res, 'skopt_result.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(dice_coeffs)[:50] # Indices of 10 worst images\n",
    "for i in indices:\n",
    "    print(f'{i}: {dice_coeffs[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Image Prediction\n",
    "img_id = 1018\n",
    "thresh = 0.85\n",
    "\n",
    "img_name = val_imgs[img_id]\n",
    "img, ann = dataset.get_example_from_img_name(img_name)\n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "y = seg_model.predict(img_batch)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(visualize_segmentations(np.repeat(img, 3, axis=-1), ann))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 0])\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 1])\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 2])\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 3])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 0] > thresh)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 1] > thresh)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 2] > thresh)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 3] > thresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save HDF5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "seg_model.save(f'seg_model_{date_str}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
