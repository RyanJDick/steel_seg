{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataset.severstal_steel_dataset import SeverstalSteelDataset\n",
    "from model.unet import build_unet_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Write better visualization code\n",
    "# - Move training into a script that can be called from the command line\n",
    "# - Read about approaches to parameter search\n",
    "# - Export model and load in Kaggle kernel\n",
    "# - Figure out why dice_coeff is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To run in half-precision mode on GPU\n",
    "# dtype='float16'\n",
    "# K.set_floatx(dtype)\n",
    "\n",
    "# # default is 1e-7 which is too small for float16.  Without adjusting the epsilon, we will get NaN predictions because of divide by zero problems\n",
    "# K.set_epsilon(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeverstalSteelDataset.init_from_config('SETTINGS.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_batches = dataset.create_dataset(dataset_type='training')\n",
    "val_data, val_batches = dataset.create_dataset(dataset_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "# def dice_coef(y_true, y_pred, smooth=1):\n",
    "#     \"\"\"\n",
    "#     Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "#          =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "#     ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "#     \"\"\"\n",
    "#     intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "#     return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "\n",
    "# https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=0.0001):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_intersection(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return intersection\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet_model(\n",
    "    img_height=cfg['IMG_HEIGHT'],\n",
    "    img_width=cfg['IMG_WIDTH'],\n",
    "    img_channels=1,\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 4),\n",
    "    num_features=[4, 4, 16, 32],\n",
    "    drop_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(alpha=0.25, gamma=2):\n",
    "    def focal_loss_with_logits(logits, targets, alpha, gamma, y_pred):\n",
    "        weight_a = alpha * (1 - y_pred) ** gamma * targets\n",
    "        weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n",
    "\n",
    "        return (tf.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(-logits)) * (weight_a + weight_b) + logits * weight_b \n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        logits = tf.log(y_pred / (1 - y_pred))\n",
    "\n",
    "        loss = focal_loss_with_logits(logits=logits, targets=y_true, alpha=alpha, gamma=gamma, y_pred=y_pred)\n",
    "\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# def weighted_cross_entropy(beta):\n",
    "#     def convert_to_logits(y_pred):\n",
    "#         # see https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/keras/backend.py#L3525\n",
    "#         y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "\n",
    "#         return tf.log(y_pred / (1 - y_pred))\n",
    "\n",
    "#     def loss(y_true, y_pred):\n",
    "#         y_pred = convert_to_logits(y_pred)\n",
    "#         loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=beta)\n",
    "\n",
    "#         return tf.reduce_mean(loss)\n",
    "\n",
    "#     return loss\n",
    "\n",
    "def weighted_binary_crossentropy(beta, from_logits=False):\n",
    "    def _weighted_binary_crossentropy(target, output):\n",
    "        # From https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/backend.py#L4213-L4243\n",
    "        if not from_logits:\n",
    "#             if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\n",
    "#                 output.op.type != 'Sigmoid'):\n",
    "#                 epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "#                 output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
    "\n",
    "#                 # Compute cross entropy from probabilities.\n",
    "#                 bce = beta * target * math_ops.log(output + epsilon())\n",
    "#                 bce += (1 - target) * math_ops.log(1 - output + epsilon())\n",
    "#                 return -bce\n",
    "#             else:\n",
    "            # When sigmoid activation function is used for output operation, we\n",
    "            # use logits from the sigmoid function directly to compute loss in order\n",
    "            # to prevent collapsing zero when training.\n",
    "            assert len(output.op.inputs) == 1\n",
    "            output = output.op.inputs[0]\n",
    "        return tf.nn.weighted_cross_entropy_with_logits(logits=output, targets=target, pos_weight=beta)\n",
    "    return _weighted_binary_crossentropy\n",
    "\n",
    "def binary_crossentropy(target, output):\n",
    "    # When sigmoid activation function is used for output operation, we\n",
    "    # use logits from the sigmoid function directly to compute loss in order\n",
    "    # to prevent collapsing zero when training.\n",
    "    assert len(output.op.inputs) == 1\n",
    "    output = output.op.inputs[0]\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss=weighted_binary_crossentropy(10.0),#'binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(), dice_coef, dice_coef_intersection])#[dice_coef, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from checkpoint\n",
    "#model.load_weights('checkpoints/cp_20190813-211504.ckpt')\n",
    "model.load_weights('checkpoints/cp_20190814-224021.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f'checkpoints/cp_{date_str}.ckpt'\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "#tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "\n",
    "logdir = \"logs/\" + date_str\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "  cp_callback \n",
    "]\n",
    "\n",
    "results = model.fit(train_data,\n",
    "                    epochs=20,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_data,\n",
    "                    steps_per_epoch=train_batches,\n",
    "                    validation_steps=val_batches,\n",
    "                    validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = train_data.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    value = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = dataset.get_image_list('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_pred, y_true):\n",
    "    y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "    \n",
    "    dice_scores = []\n",
    "    for i in range(y_pred.shape[-1]):\n",
    "        y_pred_sum = np.sum(y_pred[:, :, i])\n",
    "        y_true_sum = np.sum(y_true[:, :, i])\n",
    "        if y_pred_sum == 0 and y_true_sum == 0:\n",
    "            dice_scores.append(1.0)\n",
    "            continue\n",
    "        intersection = np.sum(y_pred[:, :, i] * y_true[:, :, i])\n",
    "        dice_scores.append(\n",
    "            2 * intersection / (y_pred_sum + y_true_sum))\n",
    "    return np.mean(dice_scores)\n",
    "\n",
    "def onehottify(x, n=None, dtype=float):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coeffs = []\n",
    "for img_name in val_imgs:\n",
    "    img, ann = dataset.get_example_from_img_name(img_name)\n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    y = model.predict(img_batch)\n",
    "    #dice_coeffs.append(dice_coeff(y[0, :, :, :], ann))\n",
    "    y_argmax = np.argmax(y, axis=-1)\n",
    "    y_one_hot = onehottify(y_argmax, 4)\n",
    "    y_one_hot[y < 0.5] = 0\n",
    "    dice_coeffs.append(dice_coeff(y_one_hot[0, :, :, :], ann))\n",
    "\n",
    "print(f'Mean dice coeff: {np.mean(dice_coeffs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am = np.argmax(y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehottify(am, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((256, 1600, 4))\n",
    "a[np.arange()]\n",
    ">>> b = np.zeros((3, 4))\n",
    ">>> b[np.arange(3), a] = 1\n",
    ">>> b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = val_imgs[2]\n",
    "img, ann = dataset.get_example_from_img_name(img_name)\n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "y = model.predict(img_batch)\n",
    "plt.imshow(np.repeat(img, 3, axis=-1))\n",
    "plt.show()\n",
    "plt.imshow(y[0, :, :, 2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tmp = y[0, :, :, 4]\n",
    "y_norm = (y_tmp - np.amin(y_tmp))\n",
    "y_norm = y_norm / np.amax(y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_norm > 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(x, y_pred, y_true):\n",
    "    x = np.repeat(x, 3, axis=-1)\n",
    "    _, axs = plt.subplots(y_pred.shape[-1] + 1, 2, figsize=(18, 10))\n",
    "    axs[0, 0].imshow(x)\n",
    "\n",
    "    cmaps = ['Reds', 'Blues', 'Greens', 'Purples']\n",
    "    for i in range(y_pred.shape[-1]):\n",
    "        #axs[i + 1, 0].imshow(x)\n",
    "        axs[i + 1, 0].imshow(y_true[:, :, i], alpha=0.4, cmap=cmaps[i])\n",
    "        axs[i + 1, 1].imshow(x)\n",
    "        axs[i + 1, 1].imshow(y_pred[:, :, i], alpha=0.4, cmap=cmaps[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(img, y_bin[0, :, :, :], ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
