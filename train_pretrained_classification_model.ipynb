{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from steel_seg.utils import print_cm\n",
    "from steel_seg.dataset.severstal_steel_dataset_generator import \\\n",
    "    SeverstalSteelDatasetGenerator\n",
    "from steel_seg.dataset.dataset_utils import load_annotations, split_data\n",
    "from steel_seg.train import (\n",
    "    class_weighted_binary_classification_crossentropy,\n",
    "    binary_accuracy_by_class)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_dict = load_annotations(cfg['TRAIN_ANNOTATIONS_FILE'])\n",
    "imgs = list(anns_dict.keys())\n",
    "\n",
    "test_imgs, val_imgs, train_imgs = split_data(imgs,\n",
    "                                             test_split=cfg['TEST_SPLIT'],\n",
    "                                             val_split=cfg['VAL_SPLIT'],\n",
    "                                             batch_size=cfg['SEGMENTATION_BATCH_SIZE'],\n",
    "                                             load_cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SeverstalSteelDatasetGenerator(\n",
    "     train_imgs,\n",
    "     is_training=True,\n",
    "     train_img_dir=cfg['TRAIN_IMAGE_DIR'],\n",
    "     train_anns_file=cfg['TRAIN_ANNOTATIONS_FILE'],\n",
    "     img_height=cfg['IMG_HEIGHT'],\n",
    "     img_width=cfg['IMG_WIDTH'],\n",
    "     num_classes=cfg['NUM_CLASSES'],\n",
    "     batch_size=cfg['SEGMENTATION_BATCH_SIZE'],\n",
    "     brightness_max_delta=cfg['BRIGHTNESS_MAX_DELTA'],\n",
    "     contrast_lower_factor=cfg['CONTRAST_LOWER_FACTOR'],\n",
    "     contrast_upper_factor=cfg['CONTRAST_UPPER_FACTOR'],\n",
    "     balance_classes=cfg['SEGMENTATION_BALANCE_CLASSES'],\n",
    "     max_oversample_rate=cfg['SEGMENTATION_MAX_OVERSAMPLE_RATE'],\n",
    "     dense_annotation=False)\n",
    "\n",
    "val_data = SeverstalSteelDatasetGenerator(\n",
    "     val_imgs,\n",
    "     is_training=False,\n",
    "     train_img_dir=cfg['TRAIN_IMAGE_DIR'],\n",
    "     train_anns_file=cfg['TRAIN_ANNOTATIONS_FILE'],\n",
    "     img_height=cfg['IMG_HEIGHT'],\n",
    "     img_width=cfg['IMG_WIDTH'],\n",
    "     num_classes=cfg['NUM_CLASSES'],\n",
    "     batch_size=cfg['SEGMENTATION_BATCH_SIZE'],\n",
    "     brightness_max_delta=None,\n",
    "     contrast_lower_factor=None,\n",
    "     contrast_upper_factor=None,\n",
    "     balance_classes=None,\n",
    "     max_oversample_rate=None,\n",
    "     dense_annotation=False)\n",
    "\n",
    "test_data = SeverstalSteelDatasetGenerator(\n",
    "     test_imgs,\n",
    "     is_training=False,\n",
    "     train_img_dir=cfg['TRAIN_IMAGE_DIR'],\n",
    "     train_anns_file=cfg['TRAIN_ANNOTATIONS_FILE'],\n",
    "     img_height=cfg['IMG_HEIGHT'],\n",
    "     img_width=cfg['IMG_WIDTH'],\n",
    "     num_classes=cfg['NUM_CLASSES'],\n",
    "     batch_size=1,\n",
    "     brightness_max_delta=None,\n",
    "     contrast_lower_factor=None,\n",
    "     contrast_upper_factor=None,\n",
    "     balance_classes=None,\n",
    "     max_oversample_rate=None,\n",
    "     dense_annotation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (cfg['IMG_HEIGHT'], cfg['IMG_WIDTH'], 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.keras.Input(shape=(cfg['IMG_HEIGHT'], cfg['IMG_WIDTH'], 1))\n",
    "# Necessary to wrap in keras.layers.Lambda so that save_model works\n",
    "x = tf.keras.layers.Lambda(lambda x: tf.tile(x / 127.5 - 1.0, [1, 1, 1, 3]))(input)\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "output = tf.keras.layers.Dense(cfg['NUM_CLASSES'], activation=tf.keras.activations.sigmoid)(x)\n",
    "model = tf.keras.Model(inputs=[input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_name = 'mobilenet_finetune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_weights = [30.0, 40.0, 10.0, 20.0]\n",
    "cls_weights = [1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.0001),#tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss=class_weighted_binary_classification_crossentropy(cls_weights),#'binary_crossentropy',\n",
    "              metrics=[\n",
    "                binary_accuracy_by_class(0),\n",
    "                binary_accuracy_by_class(1),\n",
    "                binary_accuracy_by_class(2),\n",
    "                binary_accuracy_by_class(3),\n",
    "              ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Initial Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls classification_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'classification_checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('No checkpoints found. Starting from scratch.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use new model name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint_name = 'classification'\n",
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "initial_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='auto',\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "logdir = f'logs/{checkpoint_name}-{initial_epoch}'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "    checkpoint_cb,\n",
    "]\n",
    "results = model.fit(\n",
    "    train_data,\n",
    "    epochs=400,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data,\n",
    "    steps_per_epoch=len(train_data),\n",
    "    validation_steps=len(val_data),\n",
    "    validation_freq=1,\n",
    "    initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Thresholds on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "def make_classification_predictions(imgs, dataset):\n",
    "    y_preds = np.zeros((len(imgs), 4), dtype=np.float32)\n",
    "    y_true = np.zeros((len(imgs), 4), dtype=np.uint8)\n",
    "\n",
    "    for i, img_name in enumerate(imgs):\n",
    "        img, ann = dataset.get_example_from_img_name(img_name)\n",
    "        img_batch = np.expand_dims(img, axis=0)\n",
    "        y_cls = model.predict(img_batch)\n",
    "\n",
    "        y_true[i, :] = np.amax(ann, axis=(0, 1))\n",
    "        y_preds[i, :] = y_cls[0, :]\n",
    "    return y_preds, y_true\n",
    "\n",
    "def print_classification_result_summary(y_preds, y_true, thresholds):\n",
    "    for i in range(y_true.shape[-1]):\n",
    "        y_preds_thresh = (y_preds > thresholds[i]).astype(np.uint8)\n",
    "        cm = confusion_matrix(y_true[:, i], y_preds_thresh[:, i])\n",
    "        print(f'Confusion matric for class {i}\\n(Actual labels on left)')\n",
    "        print_cm(cm, ['0', '1'])\n",
    "        cm_norm = cm / np.sum(cm, axis=-1, keepdims=True)\n",
    "        print_cm(cm_norm, ['0', '1'])\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_preds[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "                 lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC for class {i}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_true = make_classification_predictions(val_imgs, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "print_classification_result_summary(y_preds, y_true, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_true = make_classification_predictions(test_imgs, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "print_classification_result_summary(y_preds, y_true, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save HDF5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model.save(f'mobilenet_classification_model_{date_str}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
