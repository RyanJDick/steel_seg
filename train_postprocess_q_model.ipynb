{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dice Coefficient metric used for the Severstal challenge heavily penalizes a single predicted mask pixel when the ground truth is an empty mask:\n",
    " - If the ground truth mask is empty and the predicted mask is empty, dice_score = 1.0\n",
    " - If the ground truth mask is empty and the predicted mask has a single mask pixel, dice_score = 0.0\n",
    "\n",
    "This notebook trains a model to predict the Dice score that would be achieved under 2 conditions (mask and no_mask) given the prediction made by the segmentation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and load segmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Make a submission using the heuristic-based post-processing\n",
    "# Force the minimum width of predicted masks\n",
    "# Only save best-so-far checkpoints\n",
    "# Split the datasets properly!!!!!\n",
    "# seg_train, post_train, val\n",
    "# Try training to predict the better action? Maybe weight it based on the difference in score?\n",
    "#    - Might work better than training a regression model?\n",
    "# Try reducing the lr\n",
    "# Try training on thresholded input again?\n",
    "# Try q model with different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from steel_seg.utils import dice_coeff_kaggle, rle_to_dense, dense_to_rle, visualize_segmentations\n",
    "from steel_seg.dataset.severstal_steel_dataset import SeverstalSteelDataset, SeverstalSteelPostprocessDataset\n",
    "from steel_seg.model.unet import build_unet_model\n",
    "from steel_seg.model.deep_q_postprocessor import build_deep_q_model\n",
    "from steel_seg.train import class_weighted_binary_crossentropy, weighted_binary_crossentropy, dice_coef, eval, empty_mask_loss, empty_mask_accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_dataset = SeverstalSteelDataset.init_from_config('SETTINGS.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_batches = seg_dataset.create_dataset(dataset_type='training')\n",
    "val_data, val_batches = seg_dataset.create_dataset(dataset_type='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load segmentation model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy this from `train_segmentation_model.ipynb`\n",
    "seg_model = build_unet_model(\n",
    "    img_height=cfg['IMG_HEIGHT'],\n",
    "    img_width=cfg['IMG_WIDTH'],\n",
    "    img_channels=1,\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 4),\n",
    "    num_features=[8, 16, 32, 64],\n",
    "    drop_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from checkpoint\n",
    "#model.load_weights('checkpoints/cp_20190814-224021.ckpt')\n",
    "seg_model.load_weights('checkpoints/cp_20190820-085420.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Q function dataset tfrecords (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dataset = SeverstalSteelPostprocessDataset.init_from_config(\n",
    "    config_path='SETTINGS.yaml',\n",
    "    seg_model=seg_model,\n",
    "    seg_dataset=seg_dataset,\n",
    "    postprocess_thresh=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_dataset.create_tfrecords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Postprocessing Q Function Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_model = build_deep_q_model(\n",
    "    img_height=cfg['IMG_HEIGHT'],\n",
    "    img_width=cfg['IMG_WIDTH'],\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 4),\n",
    "    num_features=[4, 8, 16, 16],\n",
    "    drop_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_model.compile(optimizer=tf.train.AdamOptimizer(0.00001),\n",
    "                   loss=tf.keras.losses.MeanSquaredError(),\n",
    "                   metrics=[tf.keras.losses.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_batches = post_dataset.create_dataset(dataset_type='training')\n",
    "val_data, val_batches = post_dataset.create_dataset(dataset_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post_model.load_weights('postprocess_checkpoints/cp_20190907-171107.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f'postprocess_checkpoints/cp_{date_str}.ckpt'\n",
    "\n",
    "# Create checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, save_weights_only=True, verbose=1)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=8, monitor='val_loss')\n",
    "\n",
    "logdir = \"postprocessing_logs/\" + date_str\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "    checkpoint_cb,\n",
    "    early_stopping_cb,\n",
    "]\n",
    "\n",
    "results = post_model.fit(train_data,\n",
    "                         epochs=200,\n",
    "                         verbose=2,\n",
    "                         callbacks=callbacks,\n",
    "                         validation_data=val_data,\n",
    "                         steps_per_epoch=train_batches,\n",
    "                         validation_steps=val_batches,\n",
    "                         validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = seg_dataset.get_image_list('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steel_seg.model.unet import postprocess\n",
    "def deep_q_postprocess(y, q_scores):\n",
    "    y_post = np.copy(y)\n",
    "    for batch in range(y_post.shape[0]):\n",
    "        for cls in range(y_post.shape[-1]):\n",
    "            # If no_mask_score > mask_score\n",
    "            if q_scores[batch, cls, 1] > q_scores[batch, cls, 0]:\n",
    "                y_post[batch, :, :, cls] = 0\n",
    "    return y_post\n",
    "\n",
    "def eval(seg_model, post_model, dataset, img_list):\n",
    "    dice_coeffs = []\n",
    "    for img_name in img_list:\n",
    "        img, ann = dataset.get_example_from_img_name(img_name)\n",
    "        img_batch = np.expand_dims(img, axis=0)\n",
    "        y = seg_model.predict(img_batch)\n",
    "        y_one_hot = postprocess(y, 0.85) #TODO: add thresh to config file\n",
    "        q_scores = post_model.predict(y)\n",
    "        y_post = deep_q_postprocess(y_one_hot, q_scores)\n",
    "        dice_coeffs.append(dice_coeff_kaggle(y_post[0, :, :, :], ann))\n",
    "    mean_dice_coeff = np.mean(dice_coeffs)\n",
    "    print(f'Mean dice coeff: {mean_dice_coeff}')\n",
    "    return mean_dice_coeff, dice_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dice_coeff, dice_coeffs = eval(seg_model, post_model, seg_dataset, val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of worst images\n",
    "indices = np.argsort(dice_coeffs)[:50]\n",
    "for i in indices:\n",
    "    print(f'{i}: {dice_coeffs[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preds\n",
    "index = 824\n",
    "thresh = 0.85 #TODO: add to cfg file\n",
    "\n",
    "img_name = val_imgs[index]\n",
    "img, ann = seg_dataset.get_example_from_img_name(img_name)\n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "y = seg_model.predict(img_batch)\n",
    "y_one_hot = postprocess(y, thresh)\n",
    "q_scores = post_model.predict(y)\n",
    "y_post = deep_q_postprocess(y_one_hot, q_scores)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(visualize_segmentations(np.repeat(img, 3, axis=-1), ann))\n",
    "plt.show()\n",
    "thresh = 0.85\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 0] > thresh)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 1] > thresh)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 2] > thresh)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y[0, :, :, 3] > thresh)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y_post[0, :, :, 0])\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y_post[0, :, :, 1])\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y_post[0, :, :, 2])\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(y_post[0, :, :, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
