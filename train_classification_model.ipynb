{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from steel_seg.utils import (\n",
    "    dice_coeff_kaggle,\n",
    "    rle_to_dense,\n",
    "    dense_to_rle,\n",
    "    visualize_segmentations,\n",
    "    onehottify)\n",
    "from steel_seg.dataset.severstal_steel_dataset import SeverstalSteelDataset\n",
    "from steel_seg.model.unet import build_unet_model\n",
    "from steel_seg.model.classification_wrapper import build_classification_model\n",
    "from steel_seg.train import (\n",
    "    class_weighted_binary_classification_crossentropy,\n",
    "    binary_accuracy_by_class)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeverstalSteelDataset.init_from_config('SETTINGS.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_batches = dataset.create_dataset(dataset_type='training', dense_segmentation=False)\n",
    "val_data, val_batches = dataset.create_dataset(dataset_type='validation', dense_segmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = build_unet_model(\n",
    "    img_height=cfg['IMG_HEIGHT'],\n",
    "    img_width=cfg['IMG_WIDTH'],\n",
    "    img_channels=1,\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 4),\n",
    "    num_features=[32, 64, 128, 256],\n",
    "    drop_prob=0.5)\n",
    "model_checkpoint_name = 'deep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '20190916-092052'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('Error. No checkpoints found.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    seg_model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = build_classification_model(seg_model, 'conv2d_7', 4, 'conv2d_14')\n",
    "model_checkpoint_name = 'classifier' # TODO: cleanup this hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights = [30.0, 40.0, 10.0, 20.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.0001),\n",
    "    loss={'classification_output': class_weighted_binary_classification_crossentropy(cls_weights)},\n",
    "    metrics={'classification_output': [\n",
    "        binary_accuracy_by_class(0),\n",
    "        binary_accuracy_by_class(1),\n",
    "        binary_accuracy_by_class(2),\n",
    "        binary_accuracy_by_class(3),\n",
    "    ]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls classification_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '20190925-232140'\n",
    "#date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'classification_checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('No checkpoints found. Starting from scratch.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    cls_model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='auto',\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "logdir = f'logs/{checkpoint_name}-{initial_epoch}'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "    checkpoint_cb,\n",
    "]\n",
    "\n",
    "results = cls_model.fit(\n",
    "    train_data,\n",
    "    epochs=400,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data,\n",
    "    steps_per_epoch=train_batches,\n",
    "    validation_steps=val_batches,\n",
    "    validation_freq=3,\n",
    "    initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may have to reload the model (without compiling) before evaluating\n",
    "# The act of compiling the model with only one loss function messes things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = dataset.get_image_list('validation')\n",
    "len(val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.zeros((len(val_imgs), 4), dtype=np.float32)\n",
    "y_true = np.zeros((len(val_imgs), 4), dtype=np.uint8)\n",
    "\n",
    "for i, img_name in enumerate(val_imgs):\n",
    "    img, ann = dataset.get_example_from_img_name(img_name)\n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    y_seg, y_cls = cls_model.predict(img_batch)\n",
    "    \n",
    "    y_true[i, :] = np.amax(ann, axis=(0, 1))\n",
    "    y_preds[i, :] = y_cls[0, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "for i in range(y_true.shape[-1]):\n",
    "    y_preds_thresh = (y_preds > 0.5).astype(np.uint8)\n",
    "    cm = confusion_matrix(y_true[:, i], y_preds_thresh[:, i])\n",
    "    print('Confusion matric for class {i}\\n(Actual labels on left)')\n",
    "    print_cm(cm, ['0', '1'])\n",
    "    fpr, tpr, _ = roc_curve(y_true[:, i], y_preds[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC for class {i}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
