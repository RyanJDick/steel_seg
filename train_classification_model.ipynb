{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from steel_seg.utils import (\n",
    "    dice_coeff_kaggle,\n",
    "    rle_to_dense,\n",
    "    dense_to_rle,\n",
    "    visualize_segmentations,\n",
    "    onehottify)\n",
    "from steel_seg.dataset.severstal_steel_dataset import SeverstalSteelDataset\n",
    "from steel_seg.model.unet import build_unet_model\n",
    "from steel_seg.model.classification_wrapper import build_classification_model\n",
    "from steel_seg.train import (\n",
    "    class_weighted_binary_classification_crossentropy,\n",
    "    binary_accuracy_by_class)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for CUDA 10 or something?\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_LOSS_SCALING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.yaml') as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeverstalSteelDataset.init_from_config('SETTINGS.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_batches = dataset.create_dataset(dataset_type='training', dense_segmentation=False)\n",
    "val_data, val_batches = dataset.create_dataset(dataset_type='validation', dense_segmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = build_unet_model(\n",
    "    img_height=cfg['IMG_HEIGHT'],\n",
    "    img_width=cfg['IMG_WIDTH'],\n",
    "    img_channels=1,\n",
    "    num_classes=cfg['NUM_CLASSES'],\n",
    "    num_layers=4,\n",
    "    activation=tf.keras.activations.elu,\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_size=(3, 3),\n",
    "    pool_size=(2, 4),\n",
    "    num_features=[32, 64, 128, 256],\n",
    "    drop_prob=0.5)\n",
    "model_checkpoint_name = 'deep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '20190916-092052'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('Error. No checkpoints found.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    seg_model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = build_classification_model(seg_model, 'conv2d_7', 4, 'conv2d_14')\n",
    "model_checkpoint_name = 'classifier' # TODO: cleanup this hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_imgs = dataset.get_image_list('training')\n",
    "# class_counts = np.zeros((4,))\n",
    "# for img_name in train_imgs:\n",
    "#     img, ann = dataset.get_example_from_img_name(img_name)\n",
    "#     class_counts += np.amax(ann, axis=(0, 1))\n",
    "# np.array([len(train_imgs)] * 4) / class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_weights = [30.0, 40.0, 10.0, 20.0]\n",
    "cls_weights = [14.15538847, 53.28301887,  2.43921399, 15.62378976]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.0001),\n",
    "    loss={'classification_output': class_weighted_binary_classification_crossentropy(cls_weights)},\n",
    "    metrics={'classification_output': [\n",
    "        binary_accuracy_by_class(0),\n",
    "        binary_accuracy_by_class(1),\n",
    "        binary_accuracy_by_class(2),\n",
    "        binary_accuracy_by_class(3),\n",
    "    ]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls classification_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_str = '20190925-232140' # First try, probably not great weights\n",
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = f'{model_checkpoint_name}_{date_str}'\n",
    "checkpoint_path = f'classification_checkpoints/{checkpoint_name}/cp-{checkpoint_name}' + '-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "initial_epoch = 0\n",
    "if latest_checkpoint is None:\n",
    "    print('No checkpoints found. Starting from scratch.')\n",
    "else:\n",
    "    print(f'Loading weights from {latest_checkpoint}')\n",
    "    last_epoch = latest_checkpoint.split('-')[-1]\n",
    "    last_epoch = last_epoch.split('.')[0]\n",
    "    initial_epoch = int(last_epoch)\n",
    "    cls_model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='auto',\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "logdir = f'logs/{checkpoint_name}-{initial_epoch}'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "    checkpoint_cb,\n",
    "]\n",
    "\n",
    "results = cls_model.fit(\n",
    "    train_data,\n",
    "    epochs=400,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data,\n",
    "    steps_per_epoch=train_batches,\n",
    "    validation_steps=val_batches,\n",
    "    validation_freq=1,\n",
    "    initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may have to reload the model (without compiling) before evaluating\n",
    "# The act of compiling the model with only one loss function messes things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = dataset.get_image_list('validation')\n",
    "len(val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.zeros((len(val_imgs), 4), dtype=np.float32)\n",
    "y_true = np.zeros((len(val_imgs), 4), dtype=np.uint8)\n",
    "\n",
    "for i, img_name in enumerate(val_imgs):\n",
    "    img, ann = dataset.get_example_from_img_name(img_name)\n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    y_seg, y_cls = cls_model.predict(img_batch)\n",
    "    \n",
    "    y_true[i, :] = np.amax(ann, axis=(0, 1))\n",
    "    y_preds[i, :] = y_cls[0, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "for i in range(y_true.shape[-1]):\n",
    "    y_preds_thresh = (y_preds > thresholds[i]).astype(np.uint8)\n",
    "    cm = confusion_matrix(y_true[:, i], y_preds_thresh[:, i])\n",
    "    print('Confusion matric for class {i}\\n(Actual labels on left)')\n",
    "    print_cm(cm, ['0', '1'])\n",
    "    fpr, tpr, _ = roc_curve(y_true[:, i], y_preds[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC for class {i}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def postprocess(y, y_cls, thresh=None, cls_thresh=None, min_px_area=500):\n",
    "    if thresh is None:\n",
    "        thresh = [0.85, 0.85, 0.85, 0.85]\n",
    "    if cls_thresh is None:\n",
    "        cls_thresh = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "    # TODO: handle batches properly\n",
    "    batches, height, width, classes = y.shape\n",
    "    assert batches == 1\n",
    "    \n",
    "    y_argmax = np.argmax(y, axis=-1) # Only allow one class at each pixel\n",
    "    y_one_hot = onehottify(y_argmax, y.shape[-1], dtype=int)\n",
    "    for c in range(classes):\n",
    "        y_one_hot[:, :, :, c][y[:, :, :, c] < thresh[c]] = 0 # Background\n",
    "    \n",
    "    for c in range(classes):\n",
    "        if y_cls[0, c] < cls_thresh[c]:\n",
    "            y_one_hot[:, :, :, c] = 0\n",
    "        else:\n",
    "            num_component, component = cv2.connectedComponents(y_one_hot[0, :, :, c].astype(np.uint8))\n",
    "            for comp_idx in range(1, num_component):\n",
    "                comp_mask = (component == comp_idx)\n",
    "                if comp_mask.sum() < min_px_area:\n",
    "                    y_one_hot[0, :, :, c][comp_mask] = 0\n",
    "\n",
    "    return y_one_hot\n",
    "\n",
    "def eval(model, dataset, img_list, num_classes=4, thresh=None, cls_thresh=None, verbose=False):\n",
    "    num_empty_gt = [0] * num_classes\n",
    "    num_empty_gt_mask_pred = [0] * num_classes\n",
    "    num_mask_gt_empty_pred = [0] * num_classes\n",
    "    mask_sizes = [[] for _ in range(num_classes)]\n",
    "\n",
    "    dice_coeffs = []\n",
    "    for img_name in img_list:\n",
    "        img, ann = dataset.get_example_from_img_name(img_name)\n",
    "        img_batch = np.expand_dims(img, axis=0)\n",
    "        y, y_cls = model.predict(img_batch)\n",
    "        y_one_hot = postprocess(y, y_cls, thresh=thresh, cls_thresh=cls_thresh)\n",
    "        dice_coeffs.append(dice_coeff_kaggle(y_one_hot[0, :, :, :], ann))\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            gt_mask_size = np.count_nonzero(ann[:, :, c])\n",
    "            gt_is_empty = gt_mask_size == 0\n",
    "            pred_is_empty = np.count_nonzero(y_one_hot[0, :, :, c]) == 0\n",
    "            \n",
    "            if gt_is_empty:\n",
    "                num_empty_gt[c] += 1\n",
    "            else:\n",
    "                mask_sizes[c].append(gt_mask_size)\n",
    "\n",
    "            if gt_is_empty and not pred_is_empty:\n",
    "                num_empty_gt_mask_pred[c] += 1\n",
    "            \n",
    "            if not gt_is_empty and pred_is_empty:\n",
    "                num_mask_gt_empty_pred[c] += 1\n",
    "\n",
    "    if verbose:\n",
    "        for c in range(num_classes):\n",
    "            print(f'**** Class {c} ****')\n",
    "            print(f'Num empty gt masks: {num_empty_gt[c]}')\n",
    "            print(f'Num non-empty gt masks: {len(img_list) - num_empty_gt[c]}')\n",
    "            print(f'Num empty gt mask and non-empty pred: {num_empty_gt_mask_pred[c]} '\n",
    "                  f'({num_empty_gt_mask_pred[c] / num_empty_gt[c]})')\n",
    "            print(f'Num non-empty gt mask and empty pred: {num_mask_gt_empty_pred[c]} '\n",
    "                  f'({num_mask_gt_empty_pred[c] / (len(img_list) - num_empty_gt[c])})')\n",
    "            print(f'Mean mask size: {np.mean(mask_sizes[c])} (stddev: {np.std(mask_sizes[c])})')\n",
    "        \n",
    "    mean_dice_coeff = np.mean(dice_coeffs)\n",
    "    print(f'Mean dice coeff: {mean_dice_coeff}')\n",
    "    return mean_dice_coeff, dice_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dice_coeff, dice_coeffs = eval(cls_model,\n",
    "                                     dataset,\n",
    "                                     val_imgs,\n",
    "                                     thresh=[0.85, 0.85, 0.85, 0.85],\n",
    "                                     cls_thresh=[0.5, 0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean dice coeff: 0.9327411366608803\n",
    "#Mean dice coeff: 0.9336561477986604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(dice_coeffs)[:50] # Indices of 10 worst images\n",
    "for i in indices:\n",
    "    print(f'{i}: {dice_coeffs[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Image Prediction\n",
    "img_id = 936\n",
    "thresh = 0.85\n",
    "classification_thresh = [0.5, 0.5, 0.95, 0.5]\n",
    "\n",
    "img_name = val_imgs[img_id]\n",
    "img, ann = dataset.get_example_from_img_name(img_name)\n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "y, y_cls = cls_model.predict(img_batch)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(visualize_segmentations(np.repeat(img, 3, axis=-1), ann))\n",
    "plt.show()\n",
    "\n",
    "for i in range(4):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(y[0, :, :, i])\n",
    "    plt.show()\n",
    "print(f'Mask Classification: {y_cls[0, :]}')\n",
    "for i in range(y.shape[-1]):\n",
    "    pred = y[0, :, :, i] > thresh\n",
    "    if y_cls[0, i] < classification_thresh[i]:\n",
    "        pred[:, :] = 0\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save HDF5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "cls_model.save(f'seg_cls_model_{date_str}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
